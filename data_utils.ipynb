{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader\n",
    "from src.utils import *\n",
    "from src.process_reports import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean raw manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original manifest file\n",
    "manifest_path = '/mnt/disks/ext/data/tcga/manifests/gdc_manifest_tcga_read.tsv'\n",
    "df = pd.read_csv(manifest_path, sep='\\t')\n",
    "\n",
    "# Filter the entries\n",
    "filtered_df = df[df['filename'].str.contains('DX') & df['filename'].str.endswith('.svs') | df['filename'].str.endswith('.PDF')]\n",
    "\n",
    "# Save the new manifest file\n",
    "filtered_df.to_csv(f'{manifest_path[:-4]}_clean.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restructure tcga data folders by case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of case directories: 171\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "# define the base directory\n",
    "base_dir = '/mnt/disks/ext/data/tcga/crc/'\n",
    "# Define the pattern for the case id\n",
    "case_pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "# Iterate over all directories in the base directory\n",
    "for dirpath, dirnames, filenames in os.walk(base_dir):\n",
    "    for filename in filenames:\n",
    "        # Find the case id in the filename\n",
    "        match = re.search(case_pattern, filename)\n",
    "        if match:\n",
    "            case_id = match.group()\n",
    "            # Create a new directory for this case id, if it doesn't exist\n",
    "            new_dir = os.path.join(base_dir, case_id)\n",
    "            os.makedirs(new_dir, exist_ok=True)\n",
    "            # Move the file to the new directory\n",
    "            shutil.move(os.path.join(dirpath, filename), os.path.join(new_dir, filename))\n",
    "\n",
    "# remove empty dirs which are not case dirs\n",
    "for dir in os.listdir(base_dir):\n",
    "    if not dir.startswith('TCGA'):\n",
    "        shutil.rmtree(os.path.join(base_dir, dir))\n",
    "\n",
    "print(f\"# of case directories: {len(os.listdir(base_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract text from pdf reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "633it [00:57, 10.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract text from pdfs\n",
    "data_dir = '/mnt/disks/ext/data/tcga/crc/'\n",
    "reports_dir = 'data/tcga_crc/reports/'\n",
    "extract_text_from_pdf(data_dir, reports_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create manifest file for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCGA-BRCA/sTILs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# wsis w/ sTIL annotations: 700\n",
      "# annotated wsis in manifest: 700\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# set data dir\n",
    "data_dir = '/mnt/disks/ext/data/gdc/tcga/brca/'\n",
    " \n",
    "# stils_tcga_elg_path = 'data/stils/stils_tcga_ellogon.tsv'\n",
    "# wsis_stils_elg = pd.read_csv(stils_tcga_elg_path, sep='\\t')['wsi_id'].tolist()\n",
    "# print(f'# wsis w/ sTIL annotations: {len(wsis_stils_elg)}')\n",
    "\n",
    "# get list of files in manifest that are in data dir\n",
    "# annotated_wsis = [f for f in wsis_stils_elg if f in wsis_all]\n",
    "\n",
    "# reference_file = '/home/neil/multimodal/data/stils_tcga_brca_annotated.txt'\n",
    "# wsi_stils_feats_manifest_path = 'data/stils/wsi_stils_feats_manifest.txt'\n",
    "wsi_feats_manifest_path = 'data/wsi_feats_manifest.txt'\n",
    "\n",
    "# Initialize the list for the manifest\n",
    "# wsi_stils_annot_paths = [] \n",
    "wsi_paths = []\n",
    "\n",
    "# Loop through all case folders in data_dir\n",
    "# case_ids_annot = []\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    # Filter for .svs files\n",
    "    # all_wsi_paths = [file for file in files if file.endswith('.svs')]\n",
    "    diag_wsi_paths = [file for file in files if file.endswith('.svs') and 'DX' in file]\n",
    "    # if all_wsi_paths:\n",
    "    #     # Check if any of the .svs files are in the reference file\n",
    "    #     for wsi_path in all_wsi_paths:\n",
    "    #         if wsi_path.replace('.svs', '') in wsis_stils_elg:\n",
    "    #             wsi_stils_annot_paths.append(os.path.join(os.path.basename(root), wsi_path))\n",
    "    #             # case_ids_annot.append(os.path.basename(root))\n",
    "    #             # break\n",
    "    if diag_wsi_paths:\n",
    "        wsi_paths.extend([os.path.join(os.path.basename(root), wsi_path) for wsi_path in diag_wsi_paths])\n",
    "# print(f'# annotated cases: {len(set(case_ids_annot))}')\n",
    "\n",
    "# Save the list of files to the manifest path\n",
    "# print(f'# annotated wsis in manifest: {len(wsi_stils_annot_paths)}')\n",
    "with open(wsi_stils_feats_manifest_path, 'w') as f:\n",
    "    for item in wsi_stils_annot_paths:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(wsi_feats_manifest_path, 'w') as f:\n",
    "    for item in wsi_paths:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCGA-CRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# crc cases in pca: 594\n",
      "# cases in gdc dir: 632\n",
      "# wsis in gdc dir: 625\n",
      "# wsis in gdc dir w/ pca: 586\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/mnt/disks/ext/data/tcga/crc/'\n",
    "# load pca annotations file\n",
    "df_tcga_crc_pca = pd.read_csv('data/tcga_crc/pca/tcga_crc_pca.tsv', sep='\\t')\n",
    "print(f'# crc cases in pca: {len(df_tcga_crc_pca)}')\n",
    "\n",
    "# get list of cases and wsis in gdc dir\n",
    "tcga_crc_cases_gdc = os.listdir(data_dir)\n",
    "tcga_crc_wsis_gdc = [file for root, dirs, files in os.walk(data_dir) for file in files if file.endswith('.svs')]\n",
    "print(f'# cases in gdc dir: {len(tcga_crc_cases_gdc)}')\n",
    "print(f'# wsis in gdc dir: {len(tcga_crc_wsis_gdc)}')\n",
    "tcga_crc_wsis_gdc_pca = [file for file in tcga_crc_wsis_gdc if file[:15] in df_tcga_crc_pca['Sample ID'].tolist()]\n",
    "print(f'# wsis in gdc dir w/ pca: {len(tcga_crc_wsis_gdc_pca)}')\n",
    "\n",
    "# create manifest file for feat extraction from wsis in gdc\n",
    "tcga_crc_wsis_gdc_paths = [os.path.join(os.path.basename(root), file) for root, dirs, files in os.walk(data_dir) for file in files if file.endswith('.svs')]\n",
    "# print(tcga_coad_wsis_gdc_paths[:5])\n",
    "\n",
    "# save paths to manifest file\n",
    "with open('data/tcga_crc/wsi_feats_manifest.txt', 'w') as f:\n",
    "    for item in tcga_crc_wsis_gdc_paths:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create main data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sTILs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# for sTILs dataset\n",
    "# Load the annotations\n",
    "wsi_stils_annot = pd.read_csv('data/stils/stils_tcga_ellogon.tsv', sep='\\t')\n",
    "\n",
    "# Set data dirs\n",
    "wsi_feats_dir = 'data/wsi_feats'\n",
    "report_feats_dir = 'data/report_feats'\n",
    "\n",
    "# Initialize the dataset\n",
    "data_stils = []\n",
    "\n",
    "# Define the pattern for the case id\n",
    "case_pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "\n",
    "# Walk through the wsi_feats_dir\n",
    "for root, dirs, files in os.walk(wsi_feats_dir):\n",
    "    for wsi_feat_file in files:\n",
    "        # Check if the file is a feature file\n",
    "        if wsi_feat_file.endswith('.wsi.pt'):\n",
    "            # Extract the case id and slide id\n",
    "            case_id = wsi_feat_file.split('.wsi.pt')[0][:12]\n",
    "            slide_id = wsi_feat_file.split('.wsi.pt')[0]\n",
    "            \n",
    "            # print(f'case_id: {case_id}, slide_id: {slide_id}')\n",
    "            \n",
    "            # Find the matching row in the annotations\n",
    "            annot = wsi_stils_annot[wsi_stils_annot['wsi_id'] == slide_id]\n",
    "            split, stil_score = annot['split'].values[0], annot['stil_score'].values[0] if not annot.empty else (None, None)\n",
    "            \n",
    "            # Find the report file\n",
    "            report_feat_file = next((f for f in os.listdir(report_feats_dir) if f.startswith(case_id) and f.endswith('.report.pt')), None)\n",
    "            if report_feat_file is not None:\n",
    "                report_feat_path = os.path.join(report_feats_dir, report_feat_file)\n",
    "                wsi_feat_path = os.path.join(wsi_feats_dir, wsi_feat_file)\n",
    "                # Add the data to the dataset\n",
    "                data_stils.append([case_id, wsi_feat_path, report_feat_path, split, stil_score])\n",
    "\n",
    "\n",
    "# Convert the dataset to a DataFrame and save it to a CSV file\n",
    "df_data_stils = pd.DataFrame(data_stils, columns=['case_id', 'wsi_feat_path', 'report_feat_path', 'split', 'stil_score'])\n",
    "\n",
    "# drop rows w/ no sTIL score\n",
    "df_data_stils.dropna(subset=['stil_score'], inplace=True)\n",
    "\n",
    "# df['set'] = df['set'].replace({'Training': 'train', 'Test': 'test', 'Validation': 'val'})\n",
    "\n",
    "# bucketize sTIL scores\n",
    "df_data_stils['stil_lvl'] = df_data_stils['stil_score'].apply(lambda x: int(x // 0.1))\n",
    "\n",
    "print(f'# samples: {len(df_data_stils)}')\n",
    "df_data_stils.head()\n",
    "\n",
    "# save dataset to csv\n",
    "df_data_stils.to_csv('data/stils/data_stils.csv', index=False)\n",
    "\n",
    "# dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv w/ all ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# case ids: 600\n",
      "# wsi ids: 600\n",
      "# report ids: 607\n",
      "# samples: 576\n"
     ]
    }
   ],
   "source": [
    "# create a csv file w/ all case ids, wsi ids & report ids\n",
    "# set data dir\n",
    "wsi_feats_dir = 'data/tcga_crc/wsi_feats'\n",
    "report_feats_dir = 'data/tcga_crc/report_feats'\n",
    "# init list of case ids, wsi ids & report ids\n",
    "case_ids, wsi_ids, report_ids = [], [], []\n",
    "\n",
    "wsi_ids = [f.split('.wsi.pt')[0] for f in os.listdir(wsi_feats_dir)]\n",
    "case_ids = [f[:12] for f in wsi_ids]\n",
    "report_ids = [f.split('.report.pt')[0] for f in os.listdir(report_feats_dir)]\n",
    "\n",
    "print(f'# case ids: {len(case_ids)}')\n",
    "print(f'# wsi ids: {len(wsi_ids)}')\n",
    "print(f'# report ids: {len(report_ids)}')\n",
    "\n",
    "df_ids = pd.DataFrame(columns=['case_id', 'wsi_id', 'report_id'])\n",
    "for wsi_feat_path in os.listdir(wsi_feats_dir):\n",
    "    case_id = wsi_feat_path[:12]\n",
    "    wsi_id = wsi_feat_path.split('.wsi.pt')[0]\n",
    "    # find the matching report file in the report_feats_dir\n",
    "    report_feat_file = next((f for f in os.listdir(report_feats_dir) if f.startswith(case_id) and f.endswith('.report.pt')), None)\n",
    "    if report_feat_file is not None:\n",
    "        report_id = report_feat_file.split('.report.pt')[0]\n",
    "        df_ids.loc[len(df_ids)] = {'case_id': case_id, 'wsi_id': wsi_id, 'report_id': report_id}\n",
    "\n",
    "print(f'# samples: {len(df_ids)}')\n",
    "# print(df_ids.head())\n",
    "\n",
    "# save to csv\n",
    "df_ids.to_csv('data/tcga_crc/ids_tcga_crc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess subtype & grade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtype_grade = pd.read_csv('data/data_subtype_grade_annot.csv')\n",
    "print(df_subtype_grade.head(10))\n",
    "# get value counts for each region, localization and grade\n",
    "print(f'Value counts: {[df_subtype_grade[key].value_counts() for key in [\"region\", \"localization\", \"grade\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data using openai api\n",
    "report_data_file = 'data/data_subtype_grade_annot.csv'\n",
    "with open(report_data_file, 'r') as f:\n",
    "    report_text = f.read()\n",
    "\n",
    "# extract header\n",
    "report_header, report_body = report_text.split('\\n', 1)\n",
    "\n",
    "# construct prompt for lm\n",
    "context = '''This csv file contains annotations for cancer subtypes and grades. The file is unstructured. I want to standardize it like so:\n",
    "- for each of the 3 categories/cols (region, localization, grade), the corresponding labels should all be converted to lowercase)\n",
    "- for 'region', the valid labels are 'ductal', 'lobular', 'mixed' and 'NA'. all entries like 'intraductal' 'ductal (intraductal)' , 'Infiltrating ductal' etc. should all be converted to just 'ductal'. but if both 'lobular' and 'ductal' occur in the label, then convert it to 'mixed'. any other labels should be converted to 'NA'\n",
    "- for 'localization', the valid labels are 'in situ', 'invasive' , 'metastatic' and 'NA'. all entries like 'infiltrating', 'infiltrating/invasive ', 'infiltrating (invasive)' should be converted to just 'invasive'. any labels which are not one of 'invasive', 'in situ' or 'metastatic' should be converted to 'NA'.\n",
    "- for 'grade', the valid labels are '1', '2', '3' and 'NA'. so all labels like 'I', 'well differentiated', '1, (well differentiated)', 'grade 1', 'grade I' etc. should be converted to just '1', and the same for 2 (moderately differentiated) and 3 (poorly differentiated). If there are multiple grades (1/2) or a range (2-3), convert to the higher one. finally, all of 'insufficient information' etc. should be converted to just 'NA'. \n",
    "Based on the above instructions, please convert the labels in the csv file below to the standardized format and return the cleaned csv file as a string.\\n'''\n",
    "\n",
    "max_prompt_len = 5000\n",
    "max_context_len = max_prompt_len - len(context)\n",
    "\n",
    "# split prompt into chunks of max_context_len\n",
    "prompt_chunks = [context + report_header + '\\n' + report_body[i:i+max_context_len] for i in range(0, len(report_body), max_context_len)]\n",
    "# prompt = f'''\\n {report_text}'''\n",
    "    \n",
    "# init lm\n",
    "lm_name = \"gpt-3.5-turbo\"\n",
    "gen_args = {} # default args for generation\n",
    "\n",
    "# call openai\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "\n",
    "API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for prompt_chunk in tqdm(prompt_chunks):\n",
    "    data = {\n",
    "        \"model\": lm_name,\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt_chunk}],\n",
    "        **gen_args  # Add any additional arguments\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=data)\n",
    "    # print(response.content)\n",
    "    response.raise_for_status()\n",
    "    out = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    outputs.append(out)\n",
    "    \n",
    "# join outputs (and remove header in all but first chunk)\n",
    "output_text = outputs[0] + '\\n' + '\\n'.join([out.split('\\n', 1)[1] for out in outputs[1:]])\n",
    "print(f'cleaned annotations: {output_text}')\n",
    "\n",
    "# write cleaned data to file\n",
    "with open('data/data_subtype_grade_annot_clean.csv', 'w') as f:\n",
    "    f.write(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        case_id   region localization grade\n",
      "0  TCGA-3C-AAAU  lobular     invasive     3\n",
      "1  TCGA-3C-AALI   ductal     invasive     2\n",
      "2  TCGA-3C-AALJ   ductal     invasive     3\n",
      "3  TCGA-3C-AALK   ductal     invasive    NA\n",
      "4  TCGA-4H-AAAK  lobular     invasive     2\n",
      "5  TCGA-5L-AAT0  lobular     invasive     1\n",
      "6  TCGA-5L-AAT1  lobular     invasive     1\n",
      "7  TCGA-5T-A9QA    mixed     invasive     3\n",
      "8  TCGA-A1-A0SB       NA     invasive     1\n",
      "9  TCGA-A1-A0SD   ductal     invasive     2\n",
      "Invalid labels: Empty DataFrame\n",
      "Columns: [case_id, region, localization, grade]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df_subtype_grade = pd.read_csv('data/data_subtype_grade_annot_clean.csv', keep_default_na=False)\n",
    "print(df_subtype_grade.head(10))\n",
    "\n",
    "# validate data\n",
    "valid_region_labels = ['ductal', 'lobular', 'mixed', 'NA']\n",
    "valid_localization_labels = ['in situ', 'invasive', 'metastatic', 'NA']\n",
    "valid_grade_labels = ['1', '2', '3', 'NA']\n",
    "\n",
    "# convert all 'other/NA' to 'NA'\n",
    "# df_subtype_grade.replace('other/NA', 'NA', inplace=True)\n",
    "\n",
    "# print invalid labels & rows\n",
    "print(f'Invalid labels: {df_subtype_grade[~df_subtype_grade.region.isin(valid_region_labels) | ~df_subtype_grade.localization.isin(valid_localization_labels) | ~df_subtype_grade.grade.isin(valid_grade_labels)]}')\n",
    "\n",
    "# create a 'split' column with a random train/val/test split (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train (80%) and temp (20%)\n",
    "train_df, temp_df = train_test_split(df_subtype_grade, test_size=0.2, random_state=42)\n",
    "# Split the temp data into validation (50%) and test (50%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "# Add a 'split' column to each dataframe\n",
    "train_df['split'], val_df['split'], test_df['split'] = 'train', 'val', 'test'\n",
    "# Concatenate the dataframes back together\n",
    "df_subtype_grade = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "# print(df_subtype_grade.head(10))\n",
    "# print(f'Value counts: {[df_subtype_grade[key].value_counts() for key in [\"split\"]]}')\n",
    "\n",
    "# save cleaned data\n",
    "df_subtype_grade.to_csv('data/data_subtype_grade_annot_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msi_mantis\n",
      "MSS      468\n",
      "MSI-H     67\n",
      "MSI-L     22\n",
      "Name: count, dtype: int64\n",
      "msi_sensor\n",
      "MSS      496\n",
      "MSI-H     78\n",
      "MSI-L     10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load pca data\n",
    "df_pca = pd.read_csv('data/tcga_crc/pca/tcga_crc_pca.tsv', sep='\\t')\n",
    "# print(df_pca.columns)\n",
    "\n",
    "# keep only relevant columns\n",
    "cols_to_keep = ['Patient ID', 'Sample ID', 'Diagnosis Age',  'Cancer Type', 'TCGA PanCanAtlas Cancer Type Acronym', 'Cancer Type Detailed', 'Disease Free (Months)', 'Disease Free Status', 'Months of disease-specific survival','Disease-specific Survival status', 'Ethnicity Category', 'Fraction Genome Altered', 'MSI MANTIS Score', 'MSIsensor Score', 'Mutation Count', 'Overall Survival (Months)', 'Overall Survival Status', 'American Joint Committee on Cancer Metastasis Stage Code', 'Neoplasm Disease Lymph Node Stage American Joint Committee on Cancer Code', 'American Joint Committee on Cancer Tumor Stage Code', 'Person Neoplasm Cancer Status', 'Race Category', 'Radiation Therapy', 'Sex', 'Subtype', 'TMB (nonsynonymous)', 'Tumor Type']\n",
    "df_pca = df_pca[cols_to_keep]\n",
    "\n",
    "# rename columns\n",
    "col_names_dict = {'Patient ID': 'case_id', 'Sample ID': 'sample_id', 'Diagnosis Age': 'diag_age', 'Cancer Type': 'type', 'TCGA PanCanAtlas Cancer Type Acronym': 'type_tcga', 'Cancer Type Detailed': 'type_detailed', 'Disease Free (Months)': 'disease_free_months', 'Disease Free Status': 'disease_free_status', 'Months of disease-specific survival': 'disease_survival_months', 'Disease-specific Survival status': 'disease_survival_status', 'Ethnicity Category': 'ethnicity', 'Fraction Genome Altered': 'frac_genome_altered', 'MSI MANTIS Score': 'msi_mantis_score', 'MSIsensor Score': 'msi_sensor_score', 'Mutation Count': 'mutation_count', 'Overall Survival (Months)': 'overall_survival_months', 'Overall Survival Status': 'overall_survival_status', 'American Joint Committee on Cancer Metastasis Stage Code': 'ajcc_metastasis_stage', 'Neoplasm Disease Lymph Node Stage American Joint Committee on Cancer Code': 'ajcc_lymph_node_stage', 'American Joint Committee on Cancer Tumor Stage Code': 'ajcc_tumor_stage', 'Person Neoplasm Cancer Status': 'cancer_status', 'Race Category': 'race', 'Radiation Therapy': 'radiation_therapy', 'Sex': 'sex', 'Subtype': 'subtype', 'TMB (nonsynonymous)': 'tmb_score', 'Tumor Type': 'tumor_type'}\n",
    "\n",
    "# rename cols\n",
    "df_pca.rename(columns=col_names_dict, inplace=True)\n",
    "\n",
    "# convert msi scores to categories\n",
    "msi_mantis_thresholds = {'MSI-H': 0.6, 'MSI-L': 0.4, 'MSS': 0.0}\n",
    "msi_sensor_thresholds = {'MSI-H': 10, 'MSI-L': 4, 'MSS': 0}\n",
    "df_pca['msi_mantis'] = df_pca['msi_mantis_score'].apply(lambda x: next((k for k, v in msi_mantis_thresholds.items() if x >= v), None))\n",
    "df_pca['msi_sensor'] = df_pca['msi_sensor_score'].apply(lambda x: next((k for k, v in msi_sensor_thresholds.items() if x >= v), None))\n",
    "# show 10 random rows\n",
    "# df_pca.loc[:, ['msi_mantis_score', 'msi_mantis', 'msi_sensor_score', 'msi_sensor']].sample(10)\n",
    "print(df_pca.msi_mantis.value_counts())\n",
    "print(df_pca.msi_sensor.value_counts())\n",
    "\n",
    "\n",
    "# # save to csv\n",
    "df_pca.to_csv('data/tcga_crc/pca/tcga_crc_pca_clean.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge GDC & PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# load data\n",
    "data_dir = 'data/tcga_crc'\n",
    "df_ids = pd.read_csv(f'{data_dir}/ids_tcga_crc.csv', dtype='str')\n",
    "df_pca = pd.read_csv(f'{data_dir}/pca/tcga_crc_pca_clean.tsv', sep='\\t')\n",
    "# df_reports_sg = pd.read_csv(f'{data_dir}/subtype_grade/data_subtype_grade_annot_clean.csv', dtype='str', keep_default_na=False)\n",
    "\n",
    "# print(f'# cases in ids data: {len(df_ids.case_id)} of which {len(df_ids.case_id.unique())} are unique')\n",
    "# print(f'# cases in pca data: {len(df_pca.case_id)} of which {len(df_pca.case_id.unique())} are unique')\n",
    "# print(f'# cases in reports data: {len(df_reports_sg.case_id)} of which {len(df_reports_sg.case_id.unique())} are unique')\n",
    "# print(f'# unique cases in both pca & reports data: {len(set(df_pca.case_id.unique()).intersection(set(df_reports_sg.case_id.unique())))}')\n",
    "\n",
    "# merge reports_sg & pca data\n",
    "# df_merged = pd.merge(df_reports_sg, df_pca, on='case_id', how='inner')\n",
    "# merge w/ ids\n",
    "# df_merged = pd.merge(df_merged, df_ids, on='case_id', how='inner')\n",
    "df_merged = pd.merge(df_ids, df_pca, on='case_id', how='inner')\n",
    "# print(df_merged.info())\n",
    "\n",
    "# save dataset file\n",
    "df_merged.to_csv(f'{data_dir}/tcga_crc_gdc_pca.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
