{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader\n",
    "from src.utils import *\n",
    "from src.process_reports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure tcga data folders by case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# define the base directory\n",
    "base_dir = '/mnt/disks/ext/data/gdc/tcga/brca/'\n",
    "# Define the pattern for the case id\n",
    "pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "# Iterate over all directories in the base directory\n",
    "for dirpath, dirnames, filenames in os.walk(base_dir):\n",
    "    for filename in filenames:\n",
    "        # Find the case id in the filename\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            case_id = match.group()\n",
    "            # Create a new directory for this case id, if it doesn't exist\n",
    "            new_dir = os.path.join(base_dir, case_id)\n",
    "            os.makedirs(new_dir, exist_ok=True)\n",
    "            # Move the file to the new directory\n",
    "            shutil.move(os.path.join(dirpath, filename), os.path.join(new_dir, filename))\n",
    "\n",
    "# move other folders/files, except for the case folders, to misc folder\n",
    "# Define the pattern for the case id and the 8-digit alphanumeric\n",
    "case_pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "misc_pattern = r\"^[a-z0-9]{8}-\"\n",
    "\n",
    "# Create the 'misc' directory if it doesn't exist\n",
    "misc_dir = os.path.join(base_dir, 'misc')\n",
    "os.makedirs(misc_dir, exist_ok=True)\n",
    "\n",
    "# Create a list to store directories to be moved\n",
    "dirs_to_move = []\n",
    "\n",
    "# Generate a list of all directories in the base directory\n",
    "all_dirs = [x[0] for x in os.walk(base_dir) if not x[0].startswith(misc_dir)]\n",
    "\n",
    "# Iterate over all directories in the list\n",
    "for dirpath in all_dirs:\n",
    "    # If the directory is empty, remove it\n",
    "    if dirpath != base_dir and not any(os.scandir(dirpath)):\n",
    "        os.rmdir(dirpath)\n",
    "    else:\n",
    "        # If the directory name starts with an 8-digit alphanumeric followed by a hyphen\n",
    "        # and it's not a case directory, add it to the list of directories to be moved\n",
    "        dirname = os.path.basename(dirpath)\n",
    "        if re.match(misc_pattern, dirname) and not re.match(case_pattern, dirname):\n",
    "            dirs_to_move.append(dirpath)\n",
    "\n",
    "# Move the directories in the list to the 'misc' directory\n",
    "for dirpath in dirs_to_move:\n",
    "    if os.path.exists(dirpath):  # Check if the directory still exists\n",
    "        dest_dir = os.path.join(misc_dir, os.path.basename(dirpath))\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        shutil.move(dirpath, dest_dir)\n",
    "# count # cases in base dir\n",
    "case_count = 0\n",
    "\n",
    "# Iterate over all directories in the base directory\n",
    "for dirpath, dirnames, _ in os.walk(base_dir):\n",
    "    # If the directory name matches the case id pattern, increment the counter\n",
    "    dirname = os.path.basename(dirpath)\n",
    "    if re.match(case_pattern, dirname):\n",
    "        case_count += 1\n",
    "\n",
    "print(f\"total number of case directories: {case_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pdf to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Call the function\n",
    "extract_text_from_pdf('/mnt/disks/ext/data/gdc/tcga/brca/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create manifest file for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# wsis w/ sTIL annotations: 700\n",
      "# annotated wsis in manifest: 700\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "# set data dir\n",
    "data_dir = '/mnt/disks/ext/data/gdc/tcga/brca/'\n",
    " \n",
    "stils_tcga_elg_path = 'data/stils/stils_tcga_ellogon.tsv'\n",
    "wsis_stils_elg = pd.read_csv(stils_tcga_elg_path, sep='\\t')['wsi_id'].tolist()\n",
    "print(f'# wsis w/ sTIL annotations: {len(wsis_stils_elg)}')\n",
    "\n",
    "# get list of files in manifest that are in data dir\n",
    "# annotated_wsis = [f for f in wsis_stils_elg if f in wsis_all]\n",
    "\n",
    "# reference_file = '/home/neil/multimodal/data/stils_tcga_brca_annotated.txt'\n",
    "wsi_stils_feats_manifest_path = 'data/stils/wsi_stils_feats_manifest.txt'\n",
    "wsi_feats_manifest_path = 'data/wsi_feats_manifest.txt'\n",
    "\n",
    "# Initialize the list for the manifest\n",
    "wsi_stils_annot_paths, wsi_paths = [], []\n",
    "\n",
    "# Loop through all case folders in data_dir\n",
    "# case_ids_annot = []\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    # Filter for .svs files\n",
    "    all_wsi_paths = [file for file in files if file.endswith('.svs')]\n",
    "    diag_wsi_paths = [file for file in files if file.endswith('.svs') and 'DX1' in file]\n",
    "    if all_wsi_paths:\n",
    "        # Check if any of the .svs files are in the reference file\n",
    "        for wsi_path in all_wsi_paths:\n",
    "            if wsi_path.replace('.svs', '') in wsis_stils_elg:\n",
    "                wsi_stils_annot_paths.append(os.path.join(os.path.basename(root), wsi_path))\n",
    "                # case_ids_annot.append(os.path.basename(root))\n",
    "                # break\n",
    "    if diag_wsi_paths:\n",
    "        wsi_paths.extend([os.path.join(os.path.basename(root), wsi_path) for wsi_path in diag_wsi_paths])\n",
    "# print(f'# annotated cases: {len(set(case_ids_annot))}')\n",
    "\n",
    "# Save the list of files to the manifest path\n",
    "print(f'# annotated wsis in manifest: {len(wsi_stils_annot_paths)}')\n",
    "with open(wsi_stils_feats_manifest_path, 'w') as f:\n",
    "    for item in wsi_stils_annot_paths:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(wsi_feats_manifest_path, 'w') as f:\n",
    "    for item in wsi_paths:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy feature embs to/from case folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Copy feature embs from WSIs & reports to case folders\n",
    "# Directory where the feature embeddings are stored\n",
    "wsi_feats_dir = \"data/wsi_feats\"\n",
    "report_feats_dir = \"data/report_feats\"\n",
    "\n",
    "# Directory where the case folders are located\n",
    "dst_dir = \"/mnt/disks/ext/data/gdc/tcga/brca\"\n",
    "\n",
    "# Loop over all img feat files in the source directory\n",
    "for src_file in glob.glob(os.path.join(wsi_feats_dir, \"TCGA-*.pt\")):\n",
    "    # Extract the base name of the file\n",
    "    base_name = os.path.basename(src_file)\n",
    "\n",
    "    # Construct the destination directory path\n",
    "    dst_file = os.path.join(dst_dir, base_name[:12], base_name)\n",
    "\n",
    "    # Copy the file if it doesn't already exist\n",
    "    if not os.path.exists(dst_file):\n",
    "        shutil.copy(src_file, dst_file)\n",
    "    \n",
    "# Loop over all text feat files in the source directory\n",
    "for src_file in glob.glob(os.path.join(report_feats_dir, \"TCGA-*.pt\")):\n",
    "    # Extract the base name of the file\n",
    "    base_name = os.path.basename(src_file)\n",
    "\n",
    "    # Construct the destination directory path\n",
    "    dst_file = os.path.join(dst_dir, base_name[:12], base_name)\n",
    "\n",
    "    # Copy the file if it doesn't already exist\n",
    "    if not os.path.exists(dst_file):\n",
    "        shutil.copy(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Copy feature embs from WSIs & reports from case folders to separate folders\n",
    "# Directory where the case folders are located\n",
    "src_dir = \"/mnt/disks/ext/data/gdc/tcga/brca\"\n",
    "\n",
    "# Directory where the feature embeddings will be stored\n",
    "wsi_feats_dir = \"data/wsi_feats\"\n",
    "report_feats_dir = \"data/report_feats\"\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(wsi_feats_dir, exist_ok=True)\n",
    "os.makedirs(report_feats_dir, exist_ok=True)\n",
    "\n",
    "# Loop over all case folders in the source directory\n",
    "for case_folder in glob.glob(os.path.join(src_dir, \"TCGA-*\")):\n",
    "    # Loop over all img feat files in the case folder\n",
    "    for src_file in glob.glob(os.path.join(case_folder, \"*.wsi.pt\")):\n",
    "        # Extract the base name of the file\n",
    "        base_name = os.path.basename(src_file)\n",
    "\n",
    "        # Construct the destination file path\n",
    "        dst_file = os.path.join(wsi_feats_dir, base_name)\n",
    "\n",
    "        # Copy the file\n",
    "        shutil.copy(src_file, dst_file)\n",
    "\n",
    "    # Loop over all text feat files in the case folder\n",
    "    for src_file in glob.glob(os.path.join(case_folder, \"*.report.pt\")):\n",
    "        # Extract the base name of the file\n",
    "        base_name = os.path.basename(src_file)\n",
    "\n",
    "        # Construct the destination file path\n",
    "        dst_file = os.path.join(report_feats_dir, base_name)\n",
    "\n",
    "        # Copy the file\n",
    "        shutil.copy(src_file, dst_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create .csv file for loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# for sTILs dataset\n",
    "# Load the annotations\n",
    "wsi_stils_annot = pd.read_csv('data/stils/stils_tcga_ellogon.tsv', sep='\\t')\n",
    "\n",
    "# Set data dirs\n",
    "wsi_feats_dir = 'data/wsi_feats'\n",
    "report_feats_dir = 'data/report_feats'\n",
    "\n",
    "# Initialize the dataset\n",
    "data_stils = []\n",
    "\n",
    "# Define the pattern for the case id\n",
    "case_pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "\n",
    "# Walk through the wsi_feats_dir\n",
    "for root, dirs, files in os.walk(wsi_feats_dir):\n",
    "    for wsi_feat_file in files:\n",
    "        # Check if the file is a feature file\n",
    "        if wsi_feat_file.endswith('.wsi.pt'):\n",
    "            # Extract the case id and slide id\n",
    "            case_id = wsi_feat_file.split('.wsi.pt')[0][:12]\n",
    "            slide_id = wsi_feat_file.split('.wsi.pt')[0]\n",
    "            \n",
    "            # print(f'case_id: {case_id}, slide_id: {slide_id}')\n",
    "            \n",
    "            # Find the matching row in the annotations\n",
    "            annot = wsi_stils_annot[wsi_stils_annot['wsi_id'] == slide_id]\n",
    "            split, stil_score = annot['split'].values[0], annot['stil_score'].values[0] if not annot.empty else (None, None)\n",
    "            \n",
    "            # Find the report file\n",
    "            report_feat_file = next((f for f in os.listdir(report_feats_dir) if f.startswith(case_id) and f.endswith('.report.pt')), None)\n",
    "            if report_feat_file is not None:\n",
    "                report_feat_path = os.path.join(report_feats_dir, report_feat_file)\n",
    "                wsi_feat_path = os.path.join(wsi_feats_dir, wsi_feat_file)\n",
    "                # Add the data to the dataset\n",
    "                data_stils.append([case_id, wsi_feat_path, report_feat_path, split, stil_score])\n",
    "\n",
    "\n",
    "# Convert the dataset to a DataFrame and save it to a CSV file\n",
    "df_data_stils = pd.DataFrame(data_stils, columns=['case_id', 'wsi_feat_path', 'report_feat_path', 'split', 'stil_score'])\n",
    "\n",
    "# drop rows w/ no sTIL score\n",
    "df_data_stils.dropna(subset=['stil_score'], inplace=True)\n",
    "\n",
    "# df['set'] = df['set'].replace({'Training': 'train', 'Test': 'test', 'Validation': 'val'})\n",
    "\n",
    "# bucketize sTIL scores\n",
    "df_data_stils['stil_lvl'] = df_data_stils['stil_score'].apply(lambda x: int(x // 0.1))\n",
    "\n",
    "print(f'# samples: {len(df_data_stils)}')\n",
    "df_data_stils.head()\n",
    "\n",
    "# save dataset to csv\n",
    "df_data_stils.to_csv('data/stils/data_stils.csv', index=False)\n",
    "\n",
    "# dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1069\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# for subtype & grade dataset\n",
    "# Load the annotations\n",
    "reports_annot = pd.read_csv('data/data_subtype_grade_annot_clean.csv', dtype='str', keep_default_na=False)\n",
    "\n",
    "# Set data dirs\n",
    "wsi_feats_dir = 'data/wsi_feats'\n",
    "report_feats_dir = 'data/report_feats'\n",
    "\n",
    "# Initialize the dataset\n",
    "data_subtype_grade_annot = []\n",
    "\n",
    "# Define the pattern for the case id\n",
    "case_pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "\n",
    "for case_id in reports_annot['case_id'].unique():\n",
    "    # read subtype & grade\n",
    "    split, region, localization, grade = reports_annot[reports_annot['case_id'] == case_id][['split', 'region', 'localization', 'grade']].values[0]\n",
    "    \n",
    "    # find wsi feat file\n",
    "    wsi_feat_file = next((f for f in os.listdir(wsi_feats_dir) if f.startswith(case_id) and f.endswith('.wsi.pt')), None)\n",
    "    if wsi_feat_file is not None:\n",
    "        wsi_feat_path = os.path.join(wsi_feats_dir, wsi_feat_file)\n",
    "    \n",
    "    # Find the report file\n",
    "    report_feat_file = next((f for f in os.listdir(report_feats_dir) if f.startswith(case_id) and f.endswith('.report.pt')), None)\n",
    "    if report_feat_file is not None:\n",
    "        report_feat_path = os.path.join(report_feats_dir, report_feat_file)\n",
    "\n",
    "    # Add the data to the dataset\n",
    "    data_subtype_grade_annot.append([case_id, wsi_feat_path, report_feat_path, split, region, localization, grade])\n",
    "    \n",
    "# Convert the dataset to a DataFrame and save it to a CSV file\n",
    "df_subtype_grade = pd.DataFrame(data_subtype_grade_annot, columns=['case_id', 'wsi_feat_path', 'report_feat_path', 'split', 'region', 'localization', 'grade'], dtype=object)\n",
    "\n",
    "print(f'# samples: {len(df_subtype_grade)}')\n",
    "df_subtype_grade.head()\n",
    "\n",
    "# save dataset file\n",
    "df_subtype_grade.to_csv('data/data_subtype_grade.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename report files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_dir = 'data/reports_processed'\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(reports_dir):\n",
    "    # Check if the filename matches the pattern 'TCGA-XX-XXXX.*.txt'\n",
    "    if filename.startswith('TCGA-') and filename.endswith('.txt'):\n",
    "        # Extract the case_id (first 12 characters)\n",
    "        case_id = filename[:12]\n",
    "        new_filename = f\"{case_id}.txt\"\n",
    "        \n",
    "        # Construct the full paths for the source and destination\n",
    "        src_path = os.path.join(reports_dir, filename)\n",
    "        dest_path = os.path.join(reports_dir, new_filename)\n",
    "        \n",
    "        # Check if the destination filename already exists\n",
    "        if os.path.exists(dest_path):\n",
    "            print(f\"destination filename {dest_path} already exists!\")\n",
    "        else:\n",
    "            # Rename the file\n",
    "            os.rename(src_path, dest_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relevant info (summarize) from reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your directories\n",
    "reports_dir = 'data/reports'\n",
    "summary_dir = 'data/reports_distilled'\n",
    "res_dir = 'data/reports_residual'\n",
    "\n",
    "# Extract and save the reports\n",
    "distill_reports(reports_dir, summary_dir, res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess subtype & grade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtype_grade = pd.read_csv('data/data_subtype_grade_annot.csv')\n",
    "print(df_subtype_grade.head(10))\n",
    "# get value counts for each region, localization and grade\n",
    "print(f'Value counts: {[df_subtype_grade[key].value_counts() for key in [\"region\", \"localization\", \"grade\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data using openai api\n",
    "report_data_file = 'data/data_subtype_grade_annot.csv'\n",
    "with open(report_data_file, 'r') as f:\n",
    "    report_text = f.read()\n",
    "\n",
    "# extract header\n",
    "report_header, report_body = report_text.split('\\n', 1)\n",
    "\n",
    "# construct prompt for lm\n",
    "context = '''This csv file contains annotations for cancer subtypes and grades. The file is unstructured. I want to standardize it like so:\n",
    "- for each of the 3 categories/cols (region, localization, grade), the corresponding labels should all be converted to lowercase)\n",
    "- for 'region', the valid labels are 'ductal', 'lobular', 'mixed' and 'NA'. all entries like 'intraductal' 'ductal (intraductal)' , 'Infiltrating ductal' etc. should all be converted to just 'ductal'. but if both 'lobular' and 'ductal' occur in the label, then convert it to 'mixed'. any other labels should be converted to 'NA'\n",
    "- for 'localization', the valid labels are 'in situ', 'invasive' , 'metastatic' and 'NA'. all entries like 'infiltrating', 'infiltrating/invasive ', 'infiltrating (invasive)' should be converted to just 'invasive'. any labels which are not one of 'invasive', 'in situ' or 'metastatic' should be converted to 'NA'.\n",
    "- for 'grade', the valid labels are '1', '2', '3' and 'NA'. so all labels like 'I', 'well differentiated', '1, (well differentiated)', 'grade 1', 'grade I' etc. should be converted to just '1', and the same for 2 (moderately differentiated) and 3 (poorly differentiated). If there are multiple grades (1/2) or a range (2-3), convert to the higher one. finally, all of 'insufficient information' etc. should be converted to just 'NA'. \n",
    "Based on the above instructions, please convert the labels in the csv file below to the standardized format and return the cleaned csv file as a string.\\n'''\n",
    "\n",
    "max_prompt_len = 5000\n",
    "max_context_len = max_prompt_len - len(context)\n",
    "\n",
    "# split prompt into chunks of max_context_len\n",
    "prompt_chunks = [context + report_header + '\\n' + report_body[i:i+max_context_len] for i in range(0, len(report_body), max_context_len)]\n",
    "# prompt = f'''\\n {report_text}'''\n",
    "    \n",
    "# init lm\n",
    "lm_name = \"gpt-3.5-turbo\"\n",
    "gen_args = {} # default args for generation\n",
    "\n",
    "# call openai\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "\n",
    "API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for prompt_chunk in tqdm(prompt_chunks):\n",
    "    data = {\n",
    "        \"model\": lm_name,\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt_chunk}],\n",
    "        **gen_args  # Add any additional arguments\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=data)\n",
    "    # print(response.content)\n",
    "    response.raise_for_status()\n",
    "    out = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    outputs.append(out)\n",
    "    \n",
    "# join outputs (and remove header in all but first chunk)\n",
    "output_text = outputs[0] + '\\n' + '\\n'.join([out.split('\\n', 1)[1] for out in outputs[1:]])\n",
    "print(f'cleaned annotations: {output_text}')\n",
    "\n",
    "# write cleaned data to file\n",
    "with open('data/data_subtype_grade_annot_clean.csv', 'w') as f:\n",
    "    f.write(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        case_id   region localization grade\n",
      "0  TCGA-3C-AAAU  lobular     invasive     3\n",
      "1  TCGA-3C-AALI   ductal     invasive     2\n",
      "2  TCGA-3C-AALJ   ductal     invasive     3\n",
      "3  TCGA-3C-AALK   ductal     invasive    NA\n",
      "4  TCGA-4H-AAAK  lobular     invasive     2\n",
      "5  TCGA-5L-AAT0  lobular     invasive     1\n",
      "6  TCGA-5L-AAT1  lobular     invasive     1\n",
      "7  TCGA-5T-A9QA    mixed     invasive     3\n",
      "8  TCGA-A1-A0SB       NA     invasive     1\n",
      "9  TCGA-A1-A0SD   ductal     invasive     2\n",
      "Invalid labels: Empty DataFrame\n",
      "Columns: [case_id, region, localization, grade]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df_subtype_grade = pd.read_csv('data/data_subtype_grade_annot_clean.csv', keep_default_na=False)\n",
    "print(df_subtype_grade.head(10))\n",
    "\n",
    "# validate data\n",
    "valid_region_labels = ['ductal', 'lobular', 'mixed', 'NA']\n",
    "valid_localization_labels = ['in situ', 'invasive', 'metastatic', 'NA']\n",
    "valid_grade_labels = ['1', '2', '3', 'NA']\n",
    "\n",
    "# convert all 'other/NA' to 'NA'\n",
    "# df_subtype_grade.replace('other/NA', 'NA', inplace=True)\n",
    "\n",
    "# print invalid labels & rows\n",
    "print(f'Invalid labels: {df_subtype_grade[~df_subtype_grade.region.isin(valid_region_labels) | ~df_subtype_grade.localization.isin(valid_localization_labels) | ~df_subtype_grade.grade.isin(valid_grade_labels)]}')\n",
    "\n",
    "# create a 'split' column with a random train/val/test split (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train (80%) and temp (20%)\n",
    "train_df, temp_df = train_test_split(df_subtype_grade, test_size=0.2, random_state=42)\n",
    "# Split the temp data into validation (50%) and test (50%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "# Add a 'split' column to each dataframe\n",
    "train_df['split'], val_df['split'], test_df['split'] = 'train', 'val', 'test'\n",
    "# Concatenate the dataframes back together\n",
    "df_subtype_grade = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "# print(df_subtype_grade.head(10))\n",
    "# print(f'Value counts: {[df_subtype_grade[key].value_counts() for key in [\"split\"]]}')\n",
    "\n",
    "# save cleaned data\n",
    "df_subtype_grade.to_csv('data/data_subtype_grade_annot_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
