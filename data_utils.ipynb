{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader\n",
    "from src.utils import *\n",
    "from src.process_reports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restructure tcga data folders by case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# define the base directory\n",
    "base_dir = '/mnt/disks/ext/data/gdc/tcga/brca/'\n",
    "# Define the pattern for the case id\n",
    "pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "# Iterate over all directories in the base directory\n",
    "for dirpath, dirnames, filenames in os.walk(base_dir):\n",
    "    for filename in filenames:\n",
    "        # Find the case id in the filename\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            case_id = match.group()\n",
    "            # Create a new directory for this case id, if it doesn't exist\n",
    "            new_dir = os.path.join(base_dir, case_id)\n",
    "            os.makedirs(new_dir, exist_ok=True)\n",
    "            # Move the file to the new directory\n",
    "            shutil.move(os.path.join(dirpath, filename), os.path.join(new_dir, filename))\n",
    "\n",
    "# move other folders/files, except for the case folders, to misc folder\n",
    "# Define the pattern for the case id and the 8-digit alphanumeric\n",
    "case_pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "misc_pattern = r\"^[a-z0-9]{8}-\"\n",
    "\n",
    "# Create the 'misc' directory if it doesn't exist\n",
    "misc_dir = os.path.join(base_dir, 'misc')\n",
    "os.makedirs(misc_dir, exist_ok=True)\n",
    "\n",
    "# Create a list to store directories to be moved\n",
    "dirs_to_move = []\n",
    "\n",
    "# Generate a list of all directories in the base directory\n",
    "all_dirs = [x[0] for x in os.walk(base_dir) if not x[0].startswith(misc_dir)]\n",
    "\n",
    "# Iterate over all directories in the list\n",
    "for dirpath in all_dirs:\n",
    "    # If the directory is empty, remove it\n",
    "    if dirpath != base_dir and not any(os.scandir(dirpath)):\n",
    "        os.rmdir(dirpath)\n",
    "    else:\n",
    "        # If the directory name starts with an 8-digit alphanumeric followed by a hyphen\n",
    "        # and it's not a case directory, add it to the list of directories to be moved\n",
    "        dirname = os.path.basename(dirpath)\n",
    "        if re.match(misc_pattern, dirname) and not re.match(case_pattern, dirname):\n",
    "            dirs_to_move.append(dirpath)\n",
    "\n",
    "# Move the directories in the list to the 'misc' directory\n",
    "for dirpath in dirs_to_move:\n",
    "    if os.path.exists(dirpath):  # Check if the directory still exists\n",
    "        dest_dir = os.path.join(misc_dir, os.path.basename(dirpath))\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        shutil.move(dirpath, dest_dir)\n",
    "# count # cases in base dir\n",
    "case_count = 0\n",
    "\n",
    "# Iterate over all directories in the base directory\n",
    "for dirpath, dirnames, _ in os.walk(base_dir):\n",
    "    # If the directory name matches the case id pattern, increment the counter\n",
    "    dirname = os.path.basename(dirpath)\n",
    "    if re.match(case_pattern, dirname):\n",
    "        case_count += 1\n",
    "\n",
    "print(f\"total number of case directories: {case_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create manifest file for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# wsis w/ sTIL annotations: 700\n",
      "# annotated wsis in manifest: 700\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "# set data dir\n",
    "data_dir = '/mnt/disks/ext/data/gdc/tcga/brca/'\n",
    " \n",
    "stils_tcga_elg_path = 'data/stils/stils_tcga_ellogon.tsv'\n",
    "wsis_stils_elg = pd.read_csv(stils_tcga_elg_path, sep='\\t')['wsi_id'].tolist()\n",
    "print(f'# wsis w/ sTIL annotations: {len(wsis_stils_elg)}')\n",
    "\n",
    "# get list of files in manifest that are in data dir\n",
    "# annotated_wsis = [f for f in wsis_stils_elg if f in wsis_all]\n",
    "\n",
    "# reference_file = '/home/neil/multimodal/data/stils_tcga_brca_annotated.txt'\n",
    "wsi_stils_feats_manifest_path = 'data/stils/wsi_stils_feats_manifest.txt'\n",
    "wsi_feats_manifest_path = 'data/wsi_feats_manifest.txt'\n",
    "\n",
    "# Initialize the list for the manifest\n",
    "wsi_stils_annot_paths, wsi_paths = [], []\n",
    "\n",
    "# Loop through all case folders in data_dir\n",
    "# case_ids_annot = []\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    # Filter for .svs files\n",
    "    all_wsi_paths = [file for file in files if file.endswith('.svs')]\n",
    "    diag_wsi_paths = [file for file in files if file.endswith('.svs') and 'DX1' in file]\n",
    "    if all_wsi_paths:\n",
    "        # Check if any of the .svs files are in the reference file\n",
    "        for wsi_path in all_wsi_paths:\n",
    "            if wsi_path.replace('.svs', '') in wsis_stils_elg:\n",
    "                wsi_stils_annot_paths.append(os.path.join(os.path.basename(root), wsi_path))\n",
    "                # case_ids_annot.append(os.path.basename(root))\n",
    "                # break\n",
    "    if diag_wsi_paths:\n",
    "        wsi_paths.extend([os.path.join(os.path.basename(root), wsi_path) for wsi_path in diag_wsi_paths])\n",
    "# print(f'# annotated cases: {len(set(case_ids_annot))}')\n",
    "\n",
    "# Save the list of files to the manifest path\n",
    "print(f'# annotated wsis in manifest: {len(wsi_stils_annot_paths)}')\n",
    "with open(wsi_stils_feats_manifest_path, 'w') as f:\n",
    "    for item in wsi_stils_annot_paths:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(wsi_feats_manifest_path, 'w') as f:\n",
    "    for item in wsi_paths:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename reports & report feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create main data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sTILs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# for sTILs dataset\n",
    "# Load the annotations\n",
    "wsi_stils_annot = pd.read_csv('data/stils/stils_tcga_ellogon.tsv', sep='\\t')\n",
    "\n",
    "# Set data dirs\n",
    "wsi_feats_dir = 'data/wsi_feats'\n",
    "report_feats_dir = 'data/report_feats'\n",
    "\n",
    "# Initialize the dataset\n",
    "data_stils = []\n",
    "\n",
    "# Define the pattern for the case id\n",
    "case_pattern = r\"TCGA-\\w{2}-\\w{4}\"\n",
    "\n",
    "# Walk through the wsi_feats_dir\n",
    "for root, dirs, files in os.walk(wsi_feats_dir):\n",
    "    for wsi_feat_file in files:\n",
    "        # Check if the file is a feature file\n",
    "        if wsi_feat_file.endswith('.wsi.pt'):\n",
    "            # Extract the case id and slide id\n",
    "            case_id = wsi_feat_file.split('.wsi.pt')[0][:12]\n",
    "            slide_id = wsi_feat_file.split('.wsi.pt')[0]\n",
    "            \n",
    "            # print(f'case_id: {case_id}, slide_id: {slide_id}')\n",
    "            \n",
    "            # Find the matching row in the annotations\n",
    "            annot = wsi_stils_annot[wsi_stils_annot['wsi_id'] == slide_id]\n",
    "            split, stil_score = annot['split'].values[0], annot['stil_score'].values[0] if not annot.empty else (None, None)\n",
    "            \n",
    "            # Find the report file\n",
    "            report_feat_file = next((f for f in os.listdir(report_feats_dir) if f.startswith(case_id) and f.endswith('.report.pt')), None)\n",
    "            if report_feat_file is not None:\n",
    "                report_feat_path = os.path.join(report_feats_dir, report_feat_file)\n",
    "                wsi_feat_path = os.path.join(wsi_feats_dir, wsi_feat_file)\n",
    "                # Add the data to the dataset\n",
    "                data_stils.append([case_id, wsi_feat_path, report_feat_path, split, stil_score])\n",
    "\n",
    "\n",
    "# Convert the dataset to a DataFrame and save it to a CSV file\n",
    "df_data_stils = pd.DataFrame(data_stils, columns=['case_id', 'wsi_feat_path', 'report_feat_path', 'split', 'stil_score'])\n",
    "\n",
    "# drop rows w/ no sTIL score\n",
    "df_data_stils.dropna(subset=['stil_score'], inplace=True)\n",
    "\n",
    "# df['set'] = df['set'].replace({'Training': 'train', 'Test': 'test', 'Validation': 'val'})\n",
    "\n",
    "# bucketize sTIL scores\n",
    "df_data_stils['stil_lvl'] = df_data_stils['stil_score'].apply(lambda x: int(x // 0.1))\n",
    "\n",
    "print(f'# samples: {len(df_data_stils)}')\n",
    "df_data_stils.head()\n",
    "\n",
    "# save dataset to csv\n",
    "df_data_stils.to_csv('data/stils/data_stils.csv', index=False)\n",
    "\n",
    "# dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv w/ all ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file w/ all case ids, wsi ids & report ids\n",
    "# set data dir\n",
    "report_feats_dir = 'data/wsi_feats'\n",
    "report_feats_dir = 'data/report_feats'\n",
    "# init list of case ids, wsi ids & report ids\n",
    "case_ids, wsi_ids, report_ids = [], [], []\n",
    "\n",
    "wsi_ids = [f.split('.wsi.pt')[0] for f in os.listdir(report_feats_dir)]\n",
    "case_ids = [f[:12] for f in wsi_ids]\n",
    "report_ids = [f.split('.report.pt')[0] for f in os.listdir(report_feats_dir)]\n",
    "\n",
    "print(f'# case ids: {len(case_ids)}')\n",
    "print(f'# wsi ids: {len(wsi_ids)}')\n",
    "print(f'# report ids: {len(report_ids)}')\n",
    "\n",
    "df_ids = pd.DataFrame(columns=['case_id', 'wsi_id', 'report_id'])\n",
    "for wsi_feat_path in os.listdir(report_feats_dir):\n",
    "    case_id = wsi_feat_path[:12]\n",
    "    wsi_id = wsi_feat_path.split('.wsi.pt')[0]\n",
    "    # find the matching report file in the report_feats_dir\n",
    "    report_feat_file = next((f for f in os.listdir(report_feats_dir) if f.startswith(case_id) and f.endswith('.report.pt')), None)\n",
    "    if report_feat_file is not None:\n",
    "        report_id = report_feat_file.split('.report.pt')[0]\n",
    "        df_ids.loc[len(df_ids)] = {'case_id': case_id, 'wsi_id': wsi_id, 'report_id': report_id}\n",
    "\n",
    "print(f'# samples: {len(df_ids)}')\n",
    "# print(df_ids.head())\n",
    "\n",
    "# save to csv\n",
    "df_ids.to_csv('data/ids_tcga_brca.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess subtype & grade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtype_grade = pd.read_csv('data/data_subtype_grade_annot.csv')\n",
    "print(df_subtype_grade.head(10))\n",
    "# get value counts for each region, localization and grade\n",
    "print(f'Value counts: {[df_subtype_grade[key].value_counts() for key in [\"region\", \"localization\", \"grade\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data using openai api\n",
    "report_data_file = 'data/data_subtype_grade_annot.csv'\n",
    "with open(report_data_file, 'r') as f:\n",
    "    report_text = f.read()\n",
    "\n",
    "# extract header\n",
    "report_header, report_body = report_text.split('\\n', 1)\n",
    "\n",
    "# construct prompt for lm\n",
    "context = '''This csv file contains annotations for cancer subtypes and grades. The file is unstructured. I want to standardize it like so:\n",
    "- for each of the 3 categories/cols (region, localization, grade), the corresponding labels should all be converted to lowercase)\n",
    "- for 'region', the valid labels are 'ductal', 'lobular', 'mixed' and 'NA'. all entries like 'intraductal' 'ductal (intraductal)' , 'Infiltrating ductal' etc. should all be converted to just 'ductal'. but if both 'lobular' and 'ductal' occur in the label, then convert it to 'mixed'. any other labels should be converted to 'NA'\n",
    "- for 'localization', the valid labels are 'in situ', 'invasive' , 'metastatic' and 'NA'. all entries like 'infiltrating', 'infiltrating/invasive ', 'infiltrating (invasive)' should be converted to just 'invasive'. any labels which are not one of 'invasive', 'in situ' or 'metastatic' should be converted to 'NA'.\n",
    "- for 'grade', the valid labels are '1', '2', '3' and 'NA'. so all labels like 'I', 'well differentiated', '1, (well differentiated)', 'grade 1', 'grade I' etc. should be converted to just '1', and the same for 2 (moderately differentiated) and 3 (poorly differentiated). If there are multiple grades (1/2) or a range (2-3), convert to the higher one. finally, all of 'insufficient information' etc. should be converted to just 'NA'. \n",
    "Based on the above instructions, please convert the labels in the csv file below to the standardized format and return the cleaned csv file as a string.\\n'''\n",
    "\n",
    "max_prompt_len = 5000\n",
    "max_context_len = max_prompt_len - len(context)\n",
    "\n",
    "# split prompt into chunks of max_context_len\n",
    "prompt_chunks = [context + report_header + '\\n' + report_body[i:i+max_context_len] for i in range(0, len(report_body), max_context_len)]\n",
    "# prompt = f'''\\n {report_text}'''\n",
    "    \n",
    "# init lm\n",
    "lm_name = \"gpt-3.5-turbo\"\n",
    "gen_args = {} # default args for generation\n",
    "\n",
    "# call openai\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")\n",
    "\n",
    "API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for prompt_chunk in tqdm(prompt_chunks):\n",
    "    data = {\n",
    "        \"model\": lm_name,\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt_chunk}],\n",
    "        **gen_args  # Add any additional arguments\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=data)\n",
    "    # print(response.content)\n",
    "    response.raise_for_status()\n",
    "    out = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    outputs.append(out)\n",
    "    \n",
    "# join outputs (and remove header in all but first chunk)\n",
    "output_text = outputs[0] + '\\n' + '\\n'.join([out.split('\\n', 1)[1] for out in outputs[1:]])\n",
    "print(f'cleaned annotations: {output_text}')\n",
    "\n",
    "# write cleaned data to file\n",
    "with open('data/data_subtype_grade_annot_clean.csv', 'w') as f:\n",
    "    f.write(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        case_id   region localization grade\n",
      "0  TCGA-3C-AAAU  lobular     invasive     3\n",
      "1  TCGA-3C-AALI   ductal     invasive     2\n",
      "2  TCGA-3C-AALJ   ductal     invasive     3\n",
      "3  TCGA-3C-AALK   ductal     invasive    NA\n",
      "4  TCGA-4H-AAAK  lobular     invasive     2\n",
      "5  TCGA-5L-AAT0  lobular     invasive     1\n",
      "6  TCGA-5L-AAT1  lobular     invasive     1\n",
      "7  TCGA-5T-A9QA    mixed     invasive     3\n",
      "8  TCGA-A1-A0SB       NA     invasive     1\n",
      "9  TCGA-A1-A0SD   ductal     invasive     2\n",
      "Invalid labels: Empty DataFrame\n",
      "Columns: [case_id, region, localization, grade]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df_subtype_grade = pd.read_csv('data/data_subtype_grade_annot_clean.csv', keep_default_na=False)\n",
    "print(df_subtype_grade.head(10))\n",
    "\n",
    "# validate data\n",
    "valid_region_labels = ['ductal', 'lobular', 'mixed', 'NA']\n",
    "valid_localization_labels = ['in situ', 'invasive', 'metastatic', 'NA']\n",
    "valid_grade_labels = ['1', '2', '3', 'NA']\n",
    "\n",
    "# convert all 'other/NA' to 'NA'\n",
    "# df_subtype_grade.replace('other/NA', 'NA', inplace=True)\n",
    "\n",
    "# print invalid labels & rows\n",
    "print(f'Invalid labels: {df_subtype_grade[~df_subtype_grade.region.isin(valid_region_labels) | ~df_subtype_grade.localization.isin(valid_localization_labels) | ~df_subtype_grade.grade.isin(valid_grade_labels)]}')\n",
    "\n",
    "# create a 'split' column with a random train/val/test split (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train (80%) and temp (20%)\n",
    "train_df, temp_df = train_test_split(df_subtype_grade, test_size=0.2, random_state=42)\n",
    "# Split the temp data into validation (50%) and test (50%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "# Add a 'split' column to each dataframe\n",
    "train_df['split'], val_df['split'], test_df['split'] = 'train', 'val', 'test'\n",
    "# Concatenate the dataframes back together\n",
    "df_subtype_grade = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "# print(df_subtype_grade.head(10))\n",
    "# print(f'Value counts: {[df_subtype_grade[key].value_counts() for key in [\"split\"]]}')\n",
    "\n",
    "# save cleaned data\n",
    "df_subtype_grade.to_csv('data/data_subtype_grade_annot_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msi_mantis\n",
      "MSS      1018\n",
      "MSI-L      11\n",
      "MSI-H       5\n",
      "Name: count, dtype: int64\n",
      "msi_sensor\n",
      "MSS      1063\n",
      "MSI-L       7\n",
      "MSI-H       5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load pca data\n",
    "df_pca = pd.read_csv('data/pca/tcga_brca_pca.tsv', sep='\\t')\n",
    "# print(df_pca.columns)\n",
    "\n",
    "# keep only relevant columns\n",
    "cols_to_keep = ['Patient ID', 'Sample ID', 'Diagnosis Age',  'Cancer Type', 'TCGA PanCanAtlas Cancer Type Acronym', 'Cancer Type Detailed', 'Disease Free (Months)', 'Disease Free Status', 'Months of disease-specific survival','Disease-specific Survival status', 'Ethnicity Category', 'Fraction Genome Altered', 'MSI MANTIS Score', 'MSIsensor Score', 'Mutation Count', 'Overall Survival (Months)', 'Overall Survival Status', 'American Joint Committee on Cancer Metastasis Stage Code', 'Neoplasm Disease Lymph Node Stage American Joint Committee on Cancer Code', 'American Joint Committee on Cancer Tumor Stage Code', 'Person Neoplasm Cancer Status', 'Race Category', 'Radiation Therapy', 'Sex', 'Subtype', 'TMB (nonsynonymous)', 'Tumor Type']\n",
    "df_pca = df_pca[cols_to_keep]\n",
    "\n",
    "# rename columns\n",
    "col_names_dict = {'Patient ID': 'case_id', 'Sample ID': 'sample_id', 'Diagnosis Age': 'diag_age', 'Cancer Type': 'type', 'TCGA PanCanAtlas Cancer Type Acronym': 'type_tcga', 'Cancer Type Detailed': 'type_detailed', 'Disease Free (Months)': 'disease_free_months', 'Disease Free Status': 'disease_free_status', 'Months of disease-specific survival': 'disease_survival_months', 'Disease-specific Survival status': 'disease_survival_status', 'Ethnicity Category': 'ethnicity', 'Fraction Genome Altered': 'frac_genome_altered', 'MSI MANTIS Score': 'msi_mantis_score', 'MSIsensor Score': 'msi_sensor_score', 'Mutation Count': 'mutation_count', 'Overall Survival (Months)': 'overall_survival_months', 'Overall Survival Status': 'overall_survival_status', 'American Joint Committee on Cancer Metastasis Stage Code': 'ajcc_metastasis_stage', 'Neoplasm Disease Lymph Node Stage American Joint Committee on Cancer Code': 'ajcc_lymph_node_stage', 'American Joint Committee on Cancer Tumor Stage Code': 'ajcc_tumor_stage', 'Person Neoplasm Cancer Status': 'cancer_status', 'Race Category': 'race', 'Radiation Therapy': 'radiation_therapy', 'Sex': 'sex', 'Subtype': 'subtype', 'TMB (nonsynonymous)': 'tmb_score', 'Tumor Type': 'tumor_type'}\n",
    "\n",
    "# rename cols\n",
    "df_pca.rename(columns=col_names_dict, inplace=True)\n",
    "\n",
    "# convert msi scores to categories\n",
    "msi_mantis_thresholds = {'MSI-H': 0.6, 'MSI-L': 0.4, 'MSS': 0.0}\n",
    "msi_sensor_thresholds = {'MSI-H': 10, 'MSI-L': 4, 'MSS': 0}\n",
    "df_pca['msi_mantis'] = df_pca['msi_mantis_score'].apply(lambda x: next((k for k, v in msi_mantis_thresholds.items() if x >= v), None))\n",
    "df_pca['msi_sensor'] = df_pca['msi_sensor_score'].apply(lambda x: next((k for k, v in msi_sensor_thresholds.items() if x >= v), None))\n",
    "# show 10 random rows\n",
    "# df_pca.loc[:, ['msi_mantis_score', 'msi_mantis', 'msi_sensor_score', 'msi_sensor']].sample(10)\n",
    "print(df_pca.msi_mantis.value_counts())\n",
    "print(df_pca.msi_sensor.value_counts())\n",
    "\n",
    "\n",
    "# # save to csv\n",
    "# df_pca.to_csv('data/pca/tcga_brca_pca_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge SG & PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# load data\n",
    "df_ids = pd.read_csv('data/ids_tcga_brca.csv', dtype='str')\n",
    "df_pca = pd.read_csv('data/pca/tcga_brca_pca_clean.csv')\n",
    "df_reports_sg = pd.read_csv('data/subtype_grade/data_subtype_grade_annot_clean.csv', dtype='str', keep_default_na=False)\n",
    "\n",
    "# print(f'# cases in ids data: {len(df_ids.case_id)} of which {len(df_ids.case_id.unique())} are unique')\n",
    "# print(f'# cases in pca data: {len(df_pca.case_id)} of which {len(df_pca.case_id.unique())} are unique')\n",
    "# print(f'# cases in reports data: {len(df_reports_sg.case_id)} of which {len(df_reports_sg.case_id.unique())} are unique')\n",
    "# print(f'# unique cases in both pca & reports data: {len(set(df_pca.case_id.unique()).intersection(set(df_reports_sg.case_id.unique())))}')\n",
    "\n",
    "# merge reports_sg & pca data\n",
    "df_merged = pd.merge(df_reports_sg, df_pca, on='case_id', how='inner')\n",
    "# merge w/ ids\n",
    "df_merged = pd.merge(df_merged, df_ids, on='case_id', how='inner')\n",
    "# print(df_merged.info())\n",
    "\n",
    "# save dataset file\n",
    "df_merged.to_csv('data/data_tcga_brca_sg_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports\n",
    "reports_dir = 'data/reports'\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(reports_dir):\n",
    "    # Extract the case_id (first 12 characters)\n",
    "    case_id = filename[:12]\n",
    "    new_filename = f\"{case_id}.txt\"\n",
    "    \n",
    "    # Construct the full paths for the source and destination\n",
    "    src_path = os.path.join(reports_dir, filename)\n",
    "    dest_path = os.path.join(reports_dir, new_filename)\n",
    "    \n",
    "    # Check if the destination filename already exists\n",
    "    if os.path.exists(dest_path):\n",
    "        print(f\"destination filename {dest_path} already exists!\")\n",
    "    else:\n",
    "        # Rename the file\n",
    "        os.rename(src_path, dest_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1035\n",
      "# samples w/ msi_mantis_score: 986\n"
     ]
    }
   ],
   "source": [
    "df_tcga_brca_sg_pca = pd.read_csv('data/data_tcga_brca_sg_pca.csv')\n",
    "print(f'# samples: {len(df_tcga_brca_sg_pca)}')\n",
    "df_tcga_brca_sg_pca.dropna(subset=['msi_mantis_score'], inplace=True)\n",
    "print(f'# samples w/ msi_mantis_score: {len(df_tcga_brca_sg_pca)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
